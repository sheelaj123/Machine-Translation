{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXu96NT81txWmui1P8Smq1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheelaj123/Machine-Translation/blob/main/Encoder_Decoder_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoder-Decoder LSTM Model building.... In Machine translation\n",
        "\n",
        "##step by step: --"
      ],
      "metadata": {
        "id": "ZmsoELJi5ojo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uYe8fBvSRrs",
        "outputId": "7aec5755-7c91-4f01-c5f7-b083d43ecb29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxPd6GpNTm0A",
        "outputId": "8267c3c3-903f-463f-b551-39f26e93f5d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txSQKTFFTprp",
        "outputId": "73bd3ed2-b691-4337-d429-e3aa64ee817b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Problem statement:\n",
        "\n",
        "Collection of sample English words and their equivalent Hindi words are given.\n",
        "\n",
        "The task is to train the machine on these words using LSTM architecture so that, when given an English word, we get the translated Hindi word as output."
      ],
      "metadata": {
        "id": "R31uCRMg5T4X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8gEH9MUX5Pw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset:  \n",
        "“English_Hindi.txt” dataset consisting of English and equivalent Hindi words.\n",
        "\n",
        "In this example of the model building using LSTM, we need to import the required libraries and need to install TensorFlow before executing the other parts of the code."
      ],
      "metadata": {
        "id": "z6wfMj9k5cId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing library\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.utils import *\n",
        "from keras.initializers import *\n",
        "import tensorflow as tf\n",
        "import time, random\n",
        "#from keras.optimizers.Adam import keras\n"
      ],
      "metadata": {
        "id": "8raEnGXH5riM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After importing all required libraries,\n",
        "\n",
        "we need to define the values for all hyperparameters which include batch size for training,\n",
        "\n",
        "latent dimensionality for the encoding space and also a number of samples to train on."
      ],
      "metadata": {
        "id": "ldjKaiOs51yU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "batch_size = 64\n",
        "latent_dim = 256\n",
        "num_samples = 31\n",
        "#31\n"
      ],
      "metadata": {
        "id": "4vMOrpqS5yhB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the next section of the code, the data vectorization will take place where we will read the input file which contains English sentences and their corresponding French Sentences.\n",
        "\n",
        " In this process, the text sequences are converted into featured vectors."
      ],
      "metadata": {
        "id": "dxYY5Lua6Bk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_chars = set()\n",
        "target_chars = set()\n",
        "with open(r'English_Hindi.txt', 'r',\n",
        "          encoding='utf-8') as f:lines = f.read().split('\\n')\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_chars:\n",
        "            input_chars.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_chars:\n",
        "            target_chars.add(char)\n",
        "input_chars = sorted(list(input_chars))\n",
        "target_chars = sorted(list(target_chars))\n",
        "num_encoder_tokens = len(input_chars)\n",
        "num_decoder_tokens = len(target_chars)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "#Print size\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "# Number of samples: 31\n",
        "# Number of unique input tokens: 38\n",
        "# Number of unique output tokens: 51\n",
        "# Max sequence length for inputs: 10\n",
        "# Max sequence length for outputs: 21\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Md3uqc159Ho",
        "outputId": "7887ea98-2cc4-4b36-a39e-a7b63a015109"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 31\n",
            "Number of unique input tokens: 38\n",
            "Number of unique output tokens: 51\n",
            "Max sequence length for inputs: 10\n",
            "Max sequence length for outputs: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After featured engineering,\n",
        "\n",
        "we will get the data with all features which will help us to define the input data for the encoder and decoder and the target data for the decoder."
      ],
      "metadata": {
        "id": "ctjY-O7U6dlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define data for encoder and decoder\n",
        "input_token_id = dict([(char, i) for i, char in enumerate(input_chars)])\n",
        "target_token_id = dict([(char, i) for i, char in enumerate(target_chars)])\n",
        "encoder_in_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
        "decoder_in_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_in_data[i, t, input_token_id[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_in_data[i, t, target_token_id[char]] = 1.\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_id[char]] = 1.\n"
      ],
      "metadata": {
        "id": "6KS5cDVB6ZKk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the next section of code,\n",
        "\n",
        "we will define the input sequence for the encoder which has been defined above and process this sequence. At last, we need to set up an initial state for the decoder using ‘encoder_states’."
      ],
      "metadata": {
        "id": "N99djT-d6n9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define and process the input sequence\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "#We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "#Using `encoder_states` set up the decoder as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n"
      ],
      "metadata": {
        "id": "Atg1EGxS6j58"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will be defining the code for the final model which\n",
        "\n",
        "will accept ‘encoder_inputs’ and ‘decoder_inputs’ as input parameters and ‘decoder_outputs’ as target parameters.\n",
        "\n",
        "After defining the final model, we will be checking it by its summary and data shape."
      ],
      "metadata": {
        "id": "3PPrxiIO62qC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Final model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "metadata": {
        "id": "ACbcpppZ61Pl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saoAsf1D69z5",
        "outputId": "042782af-d99c-435b-cfb7-3d4d0a35b644"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None, 38)]           0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, None, 51)]           0         []                            \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 256),                302080    ['input_1[0][0]']             \n",
            "                              (None, 256),                                                        \n",
            "                              (None, 256)]                                                        \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, None, 256),          315392    ['input_2[0][0]',             \n",
            "                              (None, 256),                           'lstm[0][1]',                \n",
            "                              (None, 256)]                           'lstm[0][2]']                \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, None, 51)             13107     ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 630579 (2.41 MB)\n",
            "Trainable params: 630579 (2.41 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model data Shape\n",
        "print(\"encoder_in_data shape:\",encoder_in_data.shape)\n",
        "print(\"decoder_in_data shape:\",decoder_in_data.shape)\n",
        "print(\"decoder_target_data shape:\",decoder_target_data.shape)\n",
        "# encoder_in_data shape: (31, 10, 38)\n",
        "# decoder_in_data shape: (31, 21, 51)\n",
        "# decoder_target_data shape: (31, 21, 51)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vybmTVBF7A4o",
        "outputId": "0900eec1-5515-44e2-dbad-f721ece82d95"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_in_data shape: (31, 10, 38)\n",
            "decoder_in_data shape: (31, 21, 51)\n",
            "decoder_target_data shape: (31, 21, 51)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we are ready with the final model,\n",
        "\n",
        "we need to compile and train the model. In this example, the model will be trained in 50 epochs only. But,\n",
        "\n",
        "we can train the model for more epochs for better accuracy."
      ],
      "metadata": {
        "id": "1kFhDjYW7KMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compiling and training the model\n",
        "import keras\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "#model.compile(optimizer=Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.001), loss='categorical_crossentropy')\n",
        "model.fit([encoder_in_data, decoder_in_data], decoder_target_data, batch_size = batch_size, epochs=50, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWFRcr0H7Eke",
        "outputId": "c1055b57-f23c-47bb-b317-1e052ddab5f4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.8480 - val_loss: 2.6958\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 1.8424 - val_loss: 2.6900\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 1.8376 - val_loss: 2.6842\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 1.8328 - val_loss: 2.6778\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 1.8276 - val_loss: 2.6699\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 1.8213 - val_loss: 2.6587\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 1.8129 - val_loss: 2.6395\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 1.7998 - val_loss: 2.5975\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 1.7742 - val_loss: 2.5215\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.7250 - val_loss: 2.5086\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 1.7040 - val_loss: 2.5113\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 1.6840 - val_loss: 2.5192\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 1.6748 - val_loss: 2.5203\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 1.6612 - val_loss: 2.5199\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.6577 - val_loss: 2.5208\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 1.6469 - val_loss: 2.5131\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 1.6462 - val_loss: 2.5168\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 1.6364 - val_loss: 2.4994\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.6338 - val_loss: 2.5113\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.6291 - val_loss: 2.4957\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.6207 - val_loss: 2.4908\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 1.6136 - val_loss: 2.4781\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.6100 - val_loss: 2.4824\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 1.6038 - val_loss: 2.4613\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 1.5966 - val_loss: 2.4856\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 1.6038 - val_loss: 2.4638\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.5888 - val_loss: 2.4683\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 1.5839 - val_loss: 2.4500\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 1.5741 - val_loss: 2.4633\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 1.5808 - val_loss: 2.4381\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.5633 - val_loss: 2.4584\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 1.5792 - val_loss: 2.4279\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.5568 - val_loss: 2.4434\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 1.5685 - val_loss: 2.4159\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 1.5497 - val_loss: 2.4485\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.5663 - val_loss: 2.4122\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.5399 - val_loss: 2.4461\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 1.5649 - val_loss: 2.4091\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 1.5334 - val_loss: 2.4298\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 1.5515 - val_loss: 2.3970\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 1.5257 - val_loss: 2.4532\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 1.5725 - val_loss: 2.4129\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 1.5353 - val_loss: 2.4007\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 1.5226 - val_loss: 2.4047\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 1.5251 - val_loss: 2.3889\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 1.5104 - val_loss: 2.4304\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 1.5505 - val_loss: 2.3850\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 1.5097 - val_loss: 2.4210\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.5367 - val_loss: 2.3809\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 1.5013 - val_loss: 2.4417\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d91ec4f2e90>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After successful training the model,\n",
        "\n",
        "we need to test the trained model with a sample model using the parameters of the above-trained model."
      ],
      "metadata": {
        "id": "5w6ZXwhn7URm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n"
      ],
      "metadata": {
        "id": "afQ6iZUY7ScT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the below lines of code,\n",
        "\n",
        "we will define the decode sequence of the text which will be passed to the model as an input sequence.\n",
        "\n",
        " The input sequence is encoded into a context vector or state vector which will be passed as an input to the decoder with the target sequence.\n",
        "\n",
        " This process will continue to generate the output until the end of the sequence."
      ],
      "metadata": {
        "id": "-K6cJADX7cYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_input_char_index = dict((i, char) for char, i in input_token_id.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_id.items())\n",
        "#Define Decode Sequence\n",
        "def decode_sequence(input_seq):\n",
        "    #Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    #Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    #Get the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_id['\\t']] = 1.\n",
        "    #Sampling loop for a batch of sequences\n",
        "    #(to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        #Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "        #Exit condition: either hit max length\n",
        "        #or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "        #Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "        #Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n"
      ],
      "metadata": {
        "id": "w708WKr07fPb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally,\n",
        "\n",
        " we will validate the model to decode the input words into the target words\n",
        "\n",
        "  in this case the model will translate the English words into equivalent Hindi words."
      ],
      "metadata": {
        "id": "DbnEPOUv7lT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in range(20):\n",
        "    input_seq = encoder_in_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEjRZx4k7jCO",
        "outputId": "b1429fa9-9429-4aca-8de8-b2593334e6ba"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 501ms/step\n",
            "1/1 [==============================] - 0s 433ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: ममा \n",
            "\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: ममा \n",
            "\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: माा \n",
            "\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: माा \n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: माा \n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: ममा \n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: ममा \n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: ममा \n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: ममा \n",
            "\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: Got it?\n",
            "Decoded sentence: ममा \n",
            "\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: I'm OK.\n",
            "Decoded sentence: ममा \n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "Input sentence: Awesome!\n",
            "Decoded sentence: ममा \n",
            "\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "-\n",
            "Input sentence: Come in.\n",
            "Decoded sentence: ममा \n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "-\n",
            "Input sentence: Get out!\n",
            "Decoded sentence: ममा \n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "-\n",
            "Input sentence: Go away!\n",
            "Decoded sentence: ममा \n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: Goodbye!\n",
            "Decoded sentence: ममा \n",
            "\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "-\n",
            "Input sentence: Perfect!\n",
            "Decoded sentence: ममा \n",
            "\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "-\n",
            "Input sentence: Perfect!\n",
            "Decoded sentence: ममा \n",
            "\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "-\n",
            "Input sentence: Welcome.\n",
            "Decoded sentence: ममा \n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: Welcome.\n",
            "Decoded sentence: ममा \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Topics ##Ends here, thanks for visiting...!!"
      ],
      "metadata": {
        "id": "CdoQ9ZOP8tZS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dlO91qiK8tCL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}